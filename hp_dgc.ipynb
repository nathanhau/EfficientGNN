{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import preprocess_linear, train_linear, test, train\n",
    "from dataloader import load_obgn, load_data, preprocess_graph, get_prep_ogbn, get_prep_pubmed\n",
    "\n",
    "from models.SGCRes import SGCRes\n",
    "from models.SSGC import SSGC\n",
    "from models.DGC import DGC\n",
    "from models.SGC import SGC\n",
    "from stacked_models import DeepLinear, LinearMLP\n",
    "from models.GCN import GCN\n",
    "from dgl.nn.pytorch.conv import SGConv\n",
    "\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "activation = nn.ReLU()\n",
    "epochs = 50\n",
    "batch_size = 10000\n",
    "lr = 0.02\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "weight_decay = 5e-4\n",
    "dataset = 'cora'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, features, labels, train_mask, val_mask, lr, weight_decay, epochs, is_linear=False):\n",
    "    t = perf_counter()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_mask = train_mask.to(device)\n",
    "    val_mask = val_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    g = g.to(device)\n",
    "    features = features.to(device)\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits = None\n",
    "        t1 = perf_counter()\n",
    "        if is_linear:\n",
    "            logits = model(features)\n",
    "        else:\n",
    "            logits = model(g, features)\n",
    "        forward_time.append(perf_counter()-t1)\n",
    "        loss = loss_fn(logits[train_mask], labels[train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        t1 = perf_counter()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        backward_time.append(perf_counter()-t1)\n",
    "        model.eval()\n",
    "        if is_linear:\n",
    "            logits = model(features)\n",
    "        else:\n",
    "            logits = model(g, features)\n",
    "        train_acc = torch.sum(logits[train_mask].argmax(1) == labels[train_mask]).item() / train_mask.sum().item()\n",
    "        val_acc = torch.sum(logits[val_mask].argmax(1) == labels[val_mask]).item() / val_mask.sum().item()\n",
    "        # test_acc = torch.sum(logits[test_mask].argmax(1) == labels[test_mask]).item() / test_mask.sum().item()\n",
    "        print(f'Epoch {epoch + 1:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}')\n",
    "\n",
    "    training_time = perf_counter()-t\n",
    "    print(f'Training time: {training_time:.4f}s, avg: {training_time/epochs:.4f}')\n",
    "    print(f'Forward: {sum(forward_time)/len(forward_time)}')\n",
    "    print(f'Backward: {sum(backward_time)/len(backward_time)}')\n",
    "\n",
    "\n",
    "def test(model, g, features, labels, mask, is_linear=False):\n",
    "    model.eval()\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = None\n",
    "        if is_linear:\n",
    "            logits = model(features)[mask]\n",
    "        else:\n",
    "            logits = model(g, features)[mask]  # only compute the evaluation set\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        acc = correct.item() * 1.0 / len(labels)\n",
    "        print(f'Test: {acc:.4f}')\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cora'\n",
    "epochs = 50\n",
    "lr = 0.01\n",
    "wd = 0.0005\n",
    "K = 250\n",
    "T = 10\n",
    "alpha=0.1\n",
    "\n",
    "graph, label, split_idx = load_data(dataset)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "graph = preprocess_graph(graph).to(device)\n",
    "label = label.to(device)\n",
    "# print(graph.ndata['feat'])\n",
    "raw_features = graph.ndata['feat'].to(device)\n",
    "\n",
    "in_feats = raw_features.shape[-1]\n",
    "n_classes = label.max().item()+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "100\n",
      "Epoch 01, Loss: 1.9461, Train: 0.3429, Val: 0.1540\n",
      "Epoch 02, Loss: 1.9408, Train: 0.7071, Val: 0.5800\n",
      "Epoch 03, Loss: 1.9356, Train: 0.7786, Val: 0.6120\n",
      "Epoch 04, Loss: 1.9305, Train: 0.7786, Val: 0.6080\n",
      "Epoch 05, Loss: 1.9254, Train: 0.8571, Val: 0.6620\n",
      "Epoch 06, Loss: 1.9206, Train: 0.8929, Val: 0.7500\n",
      "Epoch 07, Loss: 1.9158, Train: 0.8929, Val: 0.7780\n",
      "Epoch 08, Loss: 1.9112, Train: 0.9143, Val: 0.7820\n",
      "Epoch 09, Loss: 1.9068, Train: 0.9143, Val: 0.7920\n",
      "Epoch 10, Loss: 1.9025, Train: 0.8929, Val: 0.7760\n",
      "Epoch 11, Loss: 1.8984, Train: 0.8929, Val: 0.7700\n",
      "Epoch 12, Loss: 1.8944, Train: 0.8929, Val: 0.7600\n",
      "Epoch 13, Loss: 1.8906, Train: 0.9071, Val: 0.7620\n",
      "Epoch 14, Loss: 1.8869, Train: 0.9000, Val: 0.7640\n",
      "Epoch 15, Loss: 1.8833, Train: 0.9000, Val: 0.7700\n",
      "Epoch 16, Loss: 1.8799, Train: 0.9071, Val: 0.7700\n",
      "Epoch 17, Loss: 1.8766, Train: 0.9071, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8734, Train: 0.9143, Val: 0.7820\n",
      "Epoch 19, Loss: 1.8703, Train: 0.9143, Val: 0.7820\n",
      "Epoch 20, Loss: 1.8673, Train: 0.9071, Val: 0.7780\n",
      "Epoch 21, Loss: 1.8644, Train: 0.9000, Val: 0.7760\n",
      "Epoch 22, Loss: 1.8616, Train: 0.9000, Val: 0.7740\n",
      "Epoch 23, Loss: 1.8588, Train: 0.9000, Val: 0.7820\n",
      "Epoch 24, Loss: 1.8562, Train: 0.9000, Val: 0.7840\n",
      "Epoch 25, Loss: 1.8536, Train: 0.9000, Val: 0.7780\n",
      "Epoch 26, Loss: 1.8512, Train: 0.9000, Val: 0.7800\n",
      "Epoch 27, Loss: 1.8487, Train: 0.9000, Val: 0.7740\n",
      "Epoch 28, Loss: 1.8464, Train: 0.9000, Val: 0.7800\n",
      "Epoch 29, Loss: 1.8441, Train: 0.9000, Val: 0.7800\n",
      "Epoch 30, Loss: 1.8419, Train: 0.9000, Val: 0.7840\n",
      "Epoch 31, Loss: 1.8397, Train: 0.9000, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8376, Train: 0.8929, Val: 0.7880\n",
      "Epoch 33, Loss: 1.8356, Train: 0.8929, Val: 0.7880\n",
      "Epoch 34, Loss: 1.8336, Train: 0.8929, Val: 0.7880\n",
      "Epoch 35, Loss: 1.8316, Train: 0.9000, Val: 0.7880\n",
      "Epoch 36, Loss: 1.8297, Train: 0.9000, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8278, Train: 0.8929, Val: 0.7800\n",
      "Epoch 38, Loss: 1.8260, Train: 0.8929, Val: 0.7780\n",
      "Epoch 39, Loss: 1.8242, Train: 0.8929, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8225, Train: 0.8929, Val: 0.7820\n",
      "Epoch 41, Loss: 1.8208, Train: 0.8929, Val: 0.7820\n",
      "Epoch 42, Loss: 1.8191, Train: 0.8929, Val: 0.7820\n",
      "Epoch 43, Loss: 1.8175, Train: 0.8929, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8159, Train: 0.8929, Val: 0.7840\n",
      "Epoch 45, Loss: 1.8143, Train: 0.8929, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8128, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8113, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8098, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8084, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8070, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.6713s, avg: 0.0334\n",
      "Forward: 0.006988181999995504\n",
      "Backward: 0.017117790000005472\n",
      "Test: 0.7760\n",
      "101 101\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "101\n",
      "Epoch 01, Loss: 1.9464, Train: 0.2714, Val: 0.1300\n",
      "Epoch 02, Loss: 1.9410, Train: 0.8286, Val: 0.5980\n",
      "Epoch 03, Loss: 1.9358, Train: 0.6571, Val: 0.5000\n",
      "Epoch 04, Loss: 1.9307, Train: 0.7071, Val: 0.5560\n",
      "Epoch 05, Loss: 1.9257, Train: 0.7714, Val: 0.6540\n",
      "Epoch 06, Loss: 1.9208, Train: 0.8214, Val: 0.7120\n",
      "Epoch 07, Loss: 1.9161, Train: 0.8500, Val: 0.7240\n",
      "Epoch 08, Loss: 1.9115, Train: 0.8571, Val: 0.7340\n",
      "Epoch 09, Loss: 1.9070, Train: 0.9071, Val: 0.7480\n",
      "Epoch 10, Loss: 1.9027, Train: 0.9143, Val: 0.7780\n",
      "Epoch 11, Loss: 1.8986, Train: 0.9000, Val: 0.7720\n",
      "Epoch 12, Loss: 1.8946, Train: 0.8929, Val: 0.7800\n",
      "Epoch 13, Loss: 1.8908, Train: 0.8929, Val: 0.7940\n",
      "Epoch 14, Loss: 1.8871, Train: 0.9000, Val: 0.7920\n",
      "Epoch 15, Loss: 1.8835, Train: 0.9000, Val: 0.7800\n",
      "Epoch 16, Loss: 1.8801, Train: 0.9071, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8767, Train: 0.9071, Val: 0.7720\n",
      "Epoch 18, Loss: 1.8735, Train: 0.9000, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8704, Train: 0.9000, Val: 0.7760\n",
      "Epoch 20, Loss: 1.8674, Train: 0.8786, Val: 0.7740\n",
      "Epoch 21, Loss: 1.8645, Train: 0.8786, Val: 0.7740\n",
      "Epoch 22, Loss: 1.8617, Train: 0.8786, Val: 0.7740\n",
      "Epoch 23, Loss: 1.8590, Train: 0.8857, Val: 0.7680\n",
      "Epoch 24, Loss: 1.8563, Train: 0.8857, Val: 0.7660\n",
      "Epoch 25, Loss: 1.8537, Train: 0.9000, Val: 0.7760\n",
      "Epoch 26, Loss: 1.8512, Train: 0.9071, Val: 0.7840\n",
      "Epoch 27, Loss: 1.8488, Train: 0.9071, Val: 0.7880\n",
      "Epoch 28, Loss: 1.8465, Train: 0.9071, Val: 0.7920\n",
      "Epoch 29, Loss: 1.8442, Train: 0.9000, Val: 0.7920\n",
      "Epoch 30, Loss: 1.8420, Train: 0.9071, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8398, Train: 0.9000, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8377, Train: 0.9000, Val: 0.7820\n",
      "Epoch 33, Loss: 1.8356, Train: 0.9000, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8336, Train: 0.8929, Val: 0.7820\n",
      "Epoch 35, Loss: 1.8316, Train: 0.8929, Val: 0.7800\n",
      "Epoch 36, Loss: 1.8297, Train: 0.8857, Val: 0.7800\n",
      "Epoch 37, Loss: 1.8278, Train: 0.8857, Val: 0.7780\n",
      "Epoch 38, Loss: 1.8260, Train: 0.8857, Val: 0.7780\n",
      "Epoch 39, Loss: 1.8242, Train: 0.8857, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8225, Train: 0.8857, Val: 0.7800\n",
      "Epoch 41, Loss: 1.8207, Train: 0.8857, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8191, Train: 0.8929, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8174, Train: 0.8929, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8158, Train: 0.8929, Val: 0.7840\n",
      "Epoch 45, Loss: 1.8143, Train: 0.8929, Val: 0.7840\n",
      "Epoch 46, Loss: 1.8127, Train: 0.8929, Val: 0.7840\n",
      "Epoch 47, Loss: 1.8112, Train: 0.8929, Val: 0.7820\n",
      "Epoch 48, Loss: 1.8097, Train: 0.8857, Val: 0.7820\n",
      "Epoch 49, Loss: 1.8083, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8069, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.7749s, avg: 0.0355\n",
      "Forward: 0.007484719999999925\n",
      "Backward: 0.01739475800000605\n",
      "Test: 0.7800\n",
      "102 102\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "102\n",
      "Epoch 01, Loss: 1.9459, Train: 0.3571, Val: 0.3620\n",
      "Epoch 02, Loss: 1.9406, Train: 0.6857, Val: 0.4100\n",
      "Epoch 03, Loss: 1.9354, Train: 0.7143, Val: 0.4920\n",
      "Epoch 04, Loss: 1.9303, Train: 0.7714, Val: 0.5520\n",
      "Epoch 05, Loss: 1.9252, Train: 0.8286, Val: 0.6160\n",
      "Epoch 06, Loss: 1.9203, Train: 0.8500, Val: 0.6640\n",
      "Epoch 07, Loss: 1.9156, Train: 0.8786, Val: 0.7220\n",
      "Epoch 08, Loss: 1.9110, Train: 0.8929, Val: 0.7760\n",
      "Epoch 09, Loss: 1.9066, Train: 0.9071, Val: 0.8020\n",
      "Epoch 10, Loss: 1.9023, Train: 0.8929, Val: 0.8080\n",
      "Epoch 11, Loss: 1.8981, Train: 0.9000, Val: 0.8040\n",
      "Epoch 12, Loss: 1.8941, Train: 0.8929, Val: 0.7800\n",
      "Epoch 13, Loss: 1.8903, Train: 0.8857, Val: 0.7720\n",
      "Epoch 14, Loss: 1.8866, Train: 0.8929, Val: 0.7740\n",
      "Epoch 15, Loss: 1.8830, Train: 0.9000, Val: 0.7740\n",
      "Epoch 16, Loss: 1.8796, Train: 0.9071, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8763, Train: 0.9000, Val: 0.7700\n",
      "Epoch 18, Loss: 1.8731, Train: 0.9071, Val: 0.7720\n",
      "Epoch 19, Loss: 1.8700, Train: 0.9000, Val: 0.7720\n",
      "Epoch 20, Loss: 1.8670, Train: 0.9071, Val: 0.7800\n",
      "Epoch 21, Loss: 1.8641, Train: 0.9000, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8613, Train: 0.9000, Val: 0.7760\n",
      "Epoch 23, Loss: 1.8585, Train: 0.9000, Val: 0.7840\n",
      "Epoch 24, Loss: 1.8559, Train: 0.9000, Val: 0.7900\n",
      "Epoch 25, Loss: 1.8533, Train: 0.9000, Val: 0.7900\n",
      "Epoch 26, Loss: 1.8508, Train: 0.9071, Val: 0.7920\n",
      "Epoch 27, Loss: 1.8484, Train: 0.9000, Val: 0.7880\n",
      "Epoch 28, Loss: 1.8461, Train: 0.9000, Val: 0.7880\n",
      "Epoch 29, Loss: 1.8438, Train: 0.9000, Val: 0.7880\n",
      "Epoch 30, Loss: 1.8415, Train: 0.9000, Val: 0.7920\n",
      "Epoch 31, Loss: 1.8394, Train: 0.8929, Val: 0.7900\n",
      "Epoch 32, Loss: 1.8373, Train: 0.8857, Val: 0.7860\n",
      "Epoch 33, Loss: 1.8352, Train: 0.8857, Val: 0.7880\n",
      "Epoch 34, Loss: 1.8332, Train: 0.8857, Val: 0.7820\n",
      "Epoch 35, Loss: 1.8312, Train: 0.8857, Val: 0.7860\n",
      "Epoch 36, Loss: 1.8293, Train: 0.8857, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8275, Train: 0.8929, Val: 0.7880\n",
      "Epoch 38, Loss: 1.8256, Train: 0.8929, Val: 0.7880\n",
      "Epoch 39, Loss: 1.8238, Train: 0.8857, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8221, Train: 0.8857, Val: 0.7820\n",
      "Epoch 41, Loss: 1.8204, Train: 0.8857, Val: 0.7780\n",
      "Epoch 42, Loss: 1.8187, Train: 0.8857, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8171, Train: 0.8857, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8155, Train: 0.8857, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8140, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8124, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8109, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8095, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8080, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8066, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.7018s, avg: 0.0340\n",
      "Forward: 0.0074556140000004234\n",
      "Backward: 0.01696262400000023\n",
      "Test: 0.7810\n",
      "103 103\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "103\n",
      "Epoch 01, Loss: 1.9462, Train: 0.3071, Val: 0.2000\n",
      "Epoch 02, Loss: 1.9408, Train: 0.6643, Val: 0.4840\n",
      "Epoch 03, Loss: 1.9356, Train: 0.8000, Val: 0.5940\n",
      "Epoch 04, Loss: 1.9305, Train: 0.8429, Val: 0.6480\n",
      "Epoch 05, Loss: 1.9255, Train: 0.8000, Val: 0.6260\n",
      "Epoch 06, Loss: 1.9206, Train: 0.8357, Val: 0.6600\n",
      "Epoch 07, Loss: 1.9159, Train: 0.8643, Val: 0.7140\n",
      "Epoch 08, Loss: 1.9113, Train: 0.9214, Val: 0.7660\n",
      "Epoch 09, Loss: 1.9068, Train: 0.9214, Val: 0.7860\n",
      "Epoch 10, Loss: 1.9025, Train: 0.9143, Val: 0.7840\n",
      "Epoch 11, Loss: 1.8984, Train: 0.8929, Val: 0.7700\n",
      "Epoch 12, Loss: 1.8944, Train: 0.8857, Val: 0.7640\n",
      "Epoch 13, Loss: 1.8906, Train: 0.8857, Val: 0.7600\n",
      "Epoch 14, Loss: 1.8868, Train: 0.8857, Val: 0.7660\n",
      "Epoch 15, Loss: 1.8833, Train: 0.8857, Val: 0.7680\n",
      "Epoch 16, Loss: 1.8798, Train: 0.8929, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8765, Train: 0.9071, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8733, Train: 0.9071, Val: 0.7800\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9071, Val: 0.7780\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9071, Val: 0.7820\n",
      "Epoch 21, Loss: 1.8642, Train: 0.9071, Val: 0.7840\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9071, Val: 0.7820\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9071, Val: 0.7860\n",
      "Epoch 24, Loss: 1.8561, Train: 0.9071, Val: 0.7880\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9071, Val: 0.7820\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9000, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9000, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8462, Train: 0.9000, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8439, Train: 0.8929, Val: 0.7840\n",
      "Epoch 30, Loss: 1.8417, Train: 0.8929, Val: 0.7880\n",
      "Epoch 31, Loss: 1.8395, Train: 0.8929, Val: 0.7880\n",
      "Epoch 32, Loss: 1.8374, Train: 0.8857, Val: 0.7860\n",
      "Epoch 33, Loss: 1.8353, Train: 0.8857, Val: 0.7820\n",
      "Epoch 34, Loss: 1.8333, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8314, Train: 0.8929, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8294, Train: 0.9000, Val: 0.7880\n",
      "Epoch 37, Loss: 1.8276, Train: 0.9000, Val: 0.7900\n",
      "Epoch 38, Loss: 1.8258, Train: 0.8929, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8240, Train: 0.8857, Val: 0.7860\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8857, Val: 0.7860\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8857, Val: 0.7860\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8857, Val: 0.7860\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8857, Val: 0.7860\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8857, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8141, Train: 0.8857, Val: 0.7820\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8857, Val: 0.7820\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8857, Val: 0.7820\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8857, Val: 0.7820\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7820\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.9511s, avg: 0.0390\n",
      "Forward: 0.008563528000003089\n",
      "Backward: 0.01919176600000469\n",
      "Test: 0.7770\n",
      "104 104\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "104\n",
      "Epoch 01, Loss: 1.9461, Train: 0.3643, Val: 0.2080\n",
      "Epoch 02, Loss: 1.9407, Train: 0.7500, Val: 0.4980\n",
      "Epoch 03, Loss: 1.9355, Train: 0.6500, Val: 0.4060\n",
      "Epoch 04, Loss: 1.9304, Train: 0.6500, Val: 0.4220\n",
      "Epoch 05, Loss: 1.9254, Train: 0.7643, Val: 0.5240\n",
      "Epoch 06, Loss: 1.9205, Train: 0.8643, Val: 0.7040\n",
      "Epoch 07, Loss: 1.9157, Train: 0.9143, Val: 0.7720\n",
      "Epoch 08, Loss: 1.9111, Train: 0.9214, Val: 0.7980\n",
      "Epoch 09, Loss: 1.9067, Train: 0.9286, Val: 0.7920\n",
      "Epoch 10, Loss: 1.9024, Train: 0.8929, Val: 0.7840\n",
      "Epoch 11, Loss: 1.8983, Train: 0.8857, Val: 0.7740\n",
      "Epoch 12, Loss: 1.8943, Train: 0.8786, Val: 0.7560\n",
      "Epoch 13, Loss: 1.8905, Train: 0.8857, Val: 0.7480\n",
      "Epoch 14, Loss: 1.8868, Train: 0.8929, Val: 0.7560\n",
      "Epoch 15, Loss: 1.8832, Train: 0.9000, Val: 0.7640\n",
      "Epoch 16, Loss: 1.8798, Train: 0.9071, Val: 0.7720\n",
      "Epoch 17, Loss: 1.8764, Train: 0.9143, Val: 0.7760\n",
      "Epoch 18, Loss: 1.8732, Train: 0.9071, Val: 0.7820\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9071, Val: 0.7860\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9071, Val: 0.7840\n",
      "Epoch 21, Loss: 1.8643, Train: 0.9071, Val: 0.7820\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9071, Val: 0.7820\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9071, Val: 0.7840\n",
      "Epoch 24, Loss: 1.8561, Train: 0.8929, Val: 0.7840\n",
      "Epoch 25, Loss: 1.8535, Train: 0.8857, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9000, Val: 0.7800\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9071, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8463, Train: 0.9071, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8440, Train: 0.9000, Val: 0.7860\n",
      "Epoch 30, Loss: 1.8418, Train: 0.9000, Val: 0.7860\n",
      "Epoch 31, Loss: 1.8396, Train: 0.9000, Val: 0.7880\n",
      "Epoch 32, Loss: 1.8375, Train: 0.9000, Val: 0.7880\n",
      "Epoch 33, Loss: 1.8354, Train: 0.9000, Val: 0.7820\n",
      "Epoch 34, Loss: 1.8334, Train: 0.9000, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8315, Train: 0.9000, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8295, Train: 0.8929, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8277, Train: 0.8786, Val: 0.7840\n",
      "Epoch 38, Loss: 1.8259, Train: 0.8786, Val: 0.7820\n",
      "Epoch 39, Loss: 1.8241, Train: 0.8786, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8223, Train: 0.8786, Val: 0.7840\n",
      "Epoch 41, Loss: 1.8206, Train: 0.8786, Val: 0.7840\n",
      "Epoch 42, Loss: 1.8190, Train: 0.8786, Val: 0.7880\n",
      "Epoch 43, Loss: 1.8173, Train: 0.8786, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8157, Train: 0.8786, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8142, Train: 0.8786, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8126, Train: 0.8786, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8111, Train: 0.8786, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8097, Train: 0.8786, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8082, Train: 0.8786, Val: 0.7780\n",
      "Epoch 50, Loss: 1.8068, Train: 0.8786, Val: 0.7780\n",
      "Training time: 1.5562s, avg: 0.0311\n",
      "Forward: 0.0066891140000018365\n",
      "Backward: 0.015104881999994858\n",
      "Test: 0.7800\n",
      "105 105\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "105\n",
      "Epoch 01, Loss: 1.9461, Train: 0.2500, Val: 0.1400\n",
      "Epoch 02, Loss: 1.9408, Train: 0.5429, Val: 0.3280\n",
      "Epoch 03, Loss: 1.9356, Train: 0.6714, Val: 0.4160\n",
      "Epoch 04, Loss: 1.9304, Train: 0.7357, Val: 0.5100\n",
      "Epoch 05, Loss: 1.9254, Train: 0.8143, Val: 0.6000\n",
      "Epoch 06, Loss: 1.9205, Train: 0.8857, Val: 0.7300\n",
      "Epoch 07, Loss: 1.9158, Train: 0.9143, Val: 0.7940\n",
      "Epoch 08, Loss: 1.9112, Train: 0.9071, Val: 0.7900\n",
      "Epoch 09, Loss: 1.9067, Train: 0.8929, Val: 0.7840\n",
      "Epoch 10, Loss: 1.9024, Train: 0.8786, Val: 0.7860\n",
      "Epoch 11, Loss: 1.8983, Train: 0.8857, Val: 0.7740\n",
      "Epoch 12, Loss: 1.8943, Train: 0.8857, Val: 0.7660\n",
      "Epoch 13, Loss: 1.8905, Train: 0.9071, Val: 0.7680\n",
      "Epoch 14, Loss: 1.8868, Train: 0.9000, Val: 0.7700\n",
      "Epoch 15, Loss: 1.8832, Train: 0.9071, Val: 0.7720\n",
      "Epoch 16, Loss: 1.8798, Train: 0.9143, Val: 0.7740\n",
      "Epoch 17, Loss: 1.8765, Train: 0.9143, Val: 0.7740\n",
      "Epoch 18, Loss: 1.8733, Train: 0.9143, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8702, Train: 0.9143, Val: 0.7820\n",
      "Epoch 20, Loss: 1.8672, Train: 0.9071, Val: 0.7820\n",
      "Epoch 21, Loss: 1.8643, Train: 0.9071, Val: 0.7920\n",
      "Epoch 22, Loss: 1.8615, Train: 0.9000, Val: 0.7940\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9000, Val: 0.7860\n",
      "Epoch 24, Loss: 1.8561, Train: 0.9000, Val: 0.7760\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9071, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9071, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9000, Val: 0.7860\n",
      "Epoch 28, Loss: 1.8463, Train: 0.9000, Val: 0.7780\n",
      "Epoch 29, Loss: 1.8440, Train: 0.9000, Val: 0.7800\n",
      "Epoch 30, Loss: 1.8418, Train: 0.9000, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8396, Train: 0.9000, Val: 0.7820\n",
      "Epoch 32, Loss: 1.8375, Train: 0.9000, Val: 0.7800\n",
      "Epoch 33, Loss: 1.8354, Train: 0.9000, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8334, Train: 0.9000, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8315, Train: 0.9000, Val: 0.7860\n",
      "Epoch 36, Loss: 1.8295, Train: 0.8929, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8277, Train: 0.8929, Val: 0.7860\n",
      "Epoch 38, Loss: 1.8258, Train: 0.8929, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8241, Train: 0.8929, Val: 0.7860\n",
      "Epoch 40, Loss: 1.8223, Train: 0.8929, Val: 0.7820\n",
      "Epoch 41, Loss: 1.8206, Train: 0.8786, Val: 0.7780\n",
      "Epoch 42, Loss: 1.8189, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8173, Train: 0.8857, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8157, Train: 0.8929, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8142, Train: 0.8929, Val: 0.7840\n",
      "Epoch 46, Loss: 1.8126, Train: 0.8929, Val: 0.7840\n",
      "Epoch 47, Loss: 1.8111, Train: 0.8929, Val: 0.7840\n",
      "Epoch 48, Loss: 1.8097, Train: 0.8786, Val: 0.7820\n",
      "Epoch 49, Loss: 1.8082, Train: 0.8786, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8068, Train: 0.8786, Val: 0.7800\n",
      "Training time: 1.5907s, avg: 0.0318\n",
      "Forward: 0.006834879999998975\n",
      "Backward: 0.015616858000000775\n",
      "Test: 0.7760\n",
      "106 106\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "106\n",
      "Epoch 01, Loss: 1.9462, Train: 0.5429, Val: 0.4960\n",
      "Epoch 02, Loss: 1.9409, Train: 0.5286, Val: 0.2720\n",
      "Epoch 03, Loss: 1.9357, Train: 0.6429, Val: 0.3900\n",
      "Epoch 04, Loss: 1.9306, Train: 0.7714, Val: 0.5360\n",
      "Epoch 05, Loss: 1.9256, Train: 0.8571, Val: 0.6680\n",
      "Epoch 06, Loss: 1.9207, Train: 0.8929, Val: 0.7460\n",
      "Epoch 07, Loss: 1.9159, Train: 0.9071, Val: 0.7580\n",
      "Epoch 08, Loss: 1.9113, Train: 0.9071, Val: 0.7760\n",
      "Epoch 09, Loss: 1.9069, Train: 0.9071, Val: 0.7760\n",
      "Epoch 10, Loss: 1.9026, Train: 0.9071, Val: 0.7580\n",
      "Epoch 11, Loss: 1.8985, Train: 0.9000, Val: 0.7620\n",
      "Epoch 12, Loss: 1.8945, Train: 0.9071, Val: 0.7700\n",
      "Epoch 13, Loss: 1.8906, Train: 0.9143, Val: 0.7780\n",
      "Epoch 14, Loss: 1.8869, Train: 0.9071, Val: 0.7720\n",
      "Epoch 15, Loss: 1.8834, Train: 0.9000, Val: 0.7740\n",
      "Epoch 16, Loss: 1.8799, Train: 0.9000, Val: 0.7720\n",
      "Epoch 17, Loss: 1.8766, Train: 0.9000, Val: 0.7680\n",
      "Epoch 18, Loss: 1.8734, Train: 0.8857, Val: 0.7660\n",
      "Epoch 19, Loss: 1.8703, Train: 0.8857, Val: 0.7720\n",
      "Epoch 20, Loss: 1.8673, Train: 0.8929, Val: 0.7780\n",
      "Epoch 21, Loss: 1.8644, Train: 0.9071, Val: 0.7840\n",
      "Epoch 22, Loss: 1.8616, Train: 0.9071, Val: 0.7880\n",
      "Epoch 23, Loss: 1.8588, Train: 0.9000, Val: 0.7820\n",
      "Epoch 24, Loss: 1.8562, Train: 0.9071, Val: 0.7800\n",
      "Epoch 25, Loss: 1.8536, Train: 0.9000, Val: 0.7820\n",
      "Epoch 26, Loss: 1.8511, Train: 0.9000, Val: 0.7800\n",
      "Epoch 27, Loss: 1.8487, Train: 0.9000, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8464, Train: 0.9071, Val: 0.7820\n",
      "Epoch 29, Loss: 1.8441, Train: 0.8857, Val: 0.7800\n",
      "Epoch 30, Loss: 1.8419, Train: 0.8857, Val: 0.7780\n",
      "Epoch 31, Loss: 1.8397, Train: 0.8786, Val: 0.7800\n",
      "Epoch 32, Loss: 1.8376, Train: 0.8857, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8355, Train: 0.8929, Val: 0.7880\n",
      "Epoch 34, Loss: 1.8335, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8315, Train: 0.8929, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8296, Train: 0.8929, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8278, Train: 0.8929, Val: 0.7820\n",
      "Epoch 38, Loss: 1.8259, Train: 0.8929, Val: 0.7820\n",
      "Epoch 39, Loss: 1.8242, Train: 0.8929, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8224, Train: 0.8857, Val: 0.7780\n",
      "Epoch 41, Loss: 1.8207, Train: 0.8786, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8191, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8174, Train: 0.8786, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8158, Train: 0.8857, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8143, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8127, Train: 0.8929, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8112, Train: 0.8929, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8098, Train: 0.8929, Val: 0.7820\n",
      "Epoch 49, Loss: 1.8083, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8069, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.5021s, avg: 0.0300\n",
      "Forward: 0.006412903999996615\n",
      "Backward: 0.014890351999997619\n",
      "Test: 0.7770\n",
      "107 107\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "107\n",
      "Epoch 01, Loss: 1.9458, Train: 0.1714, Val: 0.0720\n",
      "Epoch 02, Loss: 1.9405, Train: 0.5857, Val: 0.3940\n",
      "Epoch 03, Loss: 1.9352, Train: 0.8286, Val: 0.6960\n",
      "Epoch 04, Loss: 1.9301, Train: 0.8357, Val: 0.7140\n",
      "Epoch 05, Loss: 1.9251, Train: 0.8571, Val: 0.7360\n",
      "Epoch 06, Loss: 1.9202, Train: 0.8786, Val: 0.7560\n",
      "Epoch 07, Loss: 1.9154, Train: 0.8786, Val: 0.7520\n",
      "Epoch 08, Loss: 1.9108, Train: 0.8786, Val: 0.7440\n",
      "Epoch 09, Loss: 1.9064, Train: 0.8786, Val: 0.7440\n",
      "Epoch 10, Loss: 1.9021, Train: 0.8786, Val: 0.7440\n",
      "Epoch 11, Loss: 1.8980, Train: 0.8786, Val: 0.7460\n",
      "Epoch 12, Loss: 1.8940, Train: 0.8857, Val: 0.7600\n",
      "Epoch 13, Loss: 1.8902, Train: 0.9000, Val: 0.7740\n",
      "Epoch 14, Loss: 1.8865, Train: 0.9071, Val: 0.7780\n",
      "Epoch 15, Loss: 1.8829, Train: 0.9071, Val: 0.7760\n",
      "Epoch 16, Loss: 1.8795, Train: 0.9071, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8762, Train: 0.9071, Val: 0.7840\n",
      "Epoch 18, Loss: 1.8730, Train: 0.9071, Val: 0.7820\n",
      "Epoch 19, Loss: 1.8699, Train: 0.9143, Val: 0.7740\n",
      "Epoch 20, Loss: 1.8669, Train: 0.9071, Val: 0.7820\n",
      "Epoch 21, Loss: 1.8640, Train: 0.9071, Val: 0.7840\n",
      "Epoch 22, Loss: 1.8612, Train: 0.8929, Val: 0.7820\n",
      "Epoch 23, Loss: 1.8585, Train: 0.8929, Val: 0.7800\n",
      "Epoch 24, Loss: 1.8558, Train: 0.8857, Val: 0.7760\n",
      "Epoch 25, Loss: 1.8533, Train: 0.8857, Val: 0.7820\n",
      "Epoch 26, Loss: 1.8508, Train: 0.9000, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8484, Train: 0.9000, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8460, Train: 0.9000, Val: 0.7820\n",
      "Epoch 29, Loss: 1.8437, Train: 0.9000, Val: 0.7800\n",
      "Epoch 30, Loss: 1.8415, Train: 0.9000, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8393, Train: 0.9000, Val: 0.7800\n",
      "Epoch 32, Loss: 1.8372, Train: 0.9000, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8351, Train: 0.8929, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8331, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8311, Train: 0.8857, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8292, Train: 0.8786, Val: 0.7820\n",
      "Epoch 37, Loss: 1.8273, Train: 0.8786, Val: 0.7820\n",
      "Epoch 38, Loss: 1.8255, Train: 0.8786, Val: 0.7800\n",
      "Epoch 39, Loss: 1.8237, Train: 0.8786, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8220, Train: 0.8786, Val: 0.7820\n",
      "Epoch 41, Loss: 1.8203, Train: 0.8929, Val: 0.7820\n",
      "Epoch 42, Loss: 1.8186, Train: 0.8929, Val: 0.7820\n",
      "Epoch 43, Loss: 1.8170, Train: 0.8929, Val: 0.7820\n",
      "Epoch 44, Loss: 1.8154, Train: 0.8929, Val: 0.7840\n",
      "Epoch 45, Loss: 1.8138, Train: 0.8929, Val: 0.7840\n",
      "Epoch 46, Loss: 1.8123, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8108, Train: 0.8786, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8093, Train: 0.8786, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8079, Train: 0.8786, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8065, Train: 0.8786, Val: 0.7800\n",
      "Training time: 1.4488s, avg: 0.0290\n",
      "Forward: 0.006633819999998423\n",
      "Backward: 0.013887026000001015\n",
      "Test: 0.7770\n",
      "108 108\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "108\n",
      "Epoch 01, Loss: 1.9462, Train: 0.2214, Val: 0.2220\n",
      "Epoch 02, Loss: 1.9408, Train: 0.6286, Val: 0.4980\n",
      "Epoch 03, Loss: 1.9356, Train: 0.7643, Val: 0.5820\n",
      "Epoch 04, Loss: 1.9304, Train: 0.7143, Val: 0.4900\n",
      "Epoch 05, Loss: 1.9254, Train: 0.7000, Val: 0.4640\n",
      "Epoch 06, Loss: 1.9206, Train: 0.7571, Val: 0.4880\n",
      "Epoch 07, Loss: 1.9158, Train: 0.8143, Val: 0.5560\n",
      "Epoch 08, Loss: 1.9112, Train: 0.8429, Val: 0.6640\n",
      "Epoch 09, Loss: 1.9068, Train: 0.8929, Val: 0.7500\n",
      "Epoch 10, Loss: 1.9025, Train: 0.9143, Val: 0.7780\n",
      "Epoch 11, Loss: 1.8983, Train: 0.8929, Val: 0.7760\n",
      "Epoch 12, Loss: 1.8944, Train: 0.8786, Val: 0.7720\n",
      "Epoch 13, Loss: 1.8905, Train: 0.8643, Val: 0.7700\n",
      "Epoch 14, Loss: 1.8868, Train: 0.8643, Val: 0.7700\n",
      "Epoch 15, Loss: 1.8833, Train: 0.8643, Val: 0.7720\n",
      "Epoch 16, Loss: 1.8798, Train: 0.8714, Val: 0.7840\n",
      "Epoch 17, Loss: 1.8765, Train: 0.8786, Val: 0.7740\n",
      "Epoch 18, Loss: 1.8732, Train: 0.9000, Val: 0.7820\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9071, Val: 0.7820\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9071, Val: 0.7780\n",
      "Epoch 21, Loss: 1.8642, Train: 0.9071, Val: 0.7780\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9071, Val: 0.7800\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9071, Val: 0.7800\n",
      "Epoch 24, Loss: 1.8560, Train: 0.9000, Val: 0.7820\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9000, Val: 0.7820\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9071, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9071, Val: 0.7880\n",
      "Epoch 28, Loss: 1.8462, Train: 0.8929, Val: 0.7880\n",
      "Epoch 29, Loss: 1.8439, Train: 0.8857, Val: 0.7920\n",
      "Epoch 30, Loss: 1.8417, Train: 0.8857, Val: 0.7920\n",
      "Epoch 31, Loss: 1.8395, Train: 0.8857, Val: 0.7920\n",
      "Epoch 32, Loss: 1.8374, Train: 0.8857, Val: 0.7920\n",
      "Epoch 33, Loss: 1.8354, Train: 0.9000, Val: 0.7880\n",
      "Epoch 34, Loss: 1.8334, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8314, Train: 0.8929, Val: 0.7820\n",
      "Epoch 36, Loss: 1.8295, Train: 0.8929, Val: 0.7820\n",
      "Epoch 37, Loss: 1.8276, Train: 0.8929, Val: 0.7860\n",
      "Epoch 38, Loss: 1.8258, Train: 0.8929, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8240, Train: 0.8929, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8223, Train: 0.8929, Val: 0.7780\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8857, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8189, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8786, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8157, Train: 0.8786, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8141, Train: 0.8786, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8126, Train: 0.8786, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8111, Train: 0.8786, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8786, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8082, Train: 0.8786, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8786, Val: 0.7800\n",
      "Training time: 1.4577s, avg: 0.0292\n",
      "Forward: 0.006726096000003281\n",
      "Backward: 0.013570226000001639\n",
      "Test: 0.7750\n",
      "109 109\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "109\n",
      "Epoch 01, Loss: 1.9459, Train: 0.3214, Val: 0.2100\n",
      "Epoch 02, Loss: 1.9405, Train: 0.6214, Val: 0.4140\n",
      "Epoch 03, Loss: 1.9353, Train: 0.5500, Val: 0.3220\n",
      "Epoch 04, Loss: 1.9302, Train: 0.7000, Val: 0.4900\n",
      "Epoch 05, Loss: 1.9252, Train: 0.8643, Val: 0.6840\n",
      "Epoch 06, Loss: 1.9203, Train: 0.8857, Val: 0.7640\n",
      "Epoch 07, Loss: 1.9155, Train: 0.9071, Val: 0.7700\n",
      "Epoch 08, Loss: 1.9109, Train: 0.9214, Val: 0.7780\n",
      "Epoch 09, Loss: 1.9065, Train: 0.9000, Val: 0.7680\n",
      "Epoch 10, Loss: 1.9022, Train: 0.9000, Val: 0.7560\n",
      "Epoch 11, Loss: 1.8981, Train: 0.9000, Val: 0.7580\n",
      "Epoch 12, Loss: 1.8941, Train: 0.9143, Val: 0.7700\n",
      "Epoch 13, Loss: 1.8903, Train: 0.9143, Val: 0.7880\n",
      "Epoch 14, Loss: 1.8865, Train: 0.9071, Val: 0.7840\n",
      "Epoch 15, Loss: 1.8830, Train: 0.9000, Val: 0.7820\n",
      "Epoch 16, Loss: 1.8795, Train: 0.9000, Val: 0.7740\n",
      "Epoch 17, Loss: 1.8762, Train: 0.9000, Val: 0.7700\n",
      "Epoch 18, Loss: 1.8730, Train: 0.9000, Val: 0.7760\n",
      "Epoch 19, Loss: 1.8699, Train: 0.9000, Val: 0.7720\n",
      "Epoch 20, Loss: 1.8669, Train: 0.9000, Val: 0.7780\n",
      "Epoch 21, Loss: 1.8640, Train: 0.9071, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8612, Train: 0.9071, Val: 0.7820\n",
      "Epoch 23, Loss: 1.8585, Train: 0.9071, Val: 0.7860\n",
      "Epoch 24, Loss: 1.8558, Train: 0.9071, Val: 0.7880\n",
      "Epoch 25, Loss: 1.8533, Train: 0.9000, Val: 0.7880\n",
      "Epoch 26, Loss: 1.8508, Train: 0.9000, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8484, Train: 0.9000, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8460, Train: 0.9000, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8437, Train: 0.9000, Val: 0.7840\n",
      "Epoch 30, Loss: 1.8415, Train: 0.9000, Val: 0.7840\n",
      "Epoch 31, Loss: 1.8393, Train: 0.9000, Val: 0.7820\n",
      "Epoch 32, Loss: 1.8372, Train: 0.9000, Val: 0.7800\n",
      "Epoch 33, Loss: 1.8352, Train: 0.9000, Val: 0.7800\n",
      "Epoch 34, Loss: 1.8332, Train: 0.9000, Val: 0.7820\n",
      "Epoch 35, Loss: 1.8312, Train: 0.9000, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8293, Train: 0.9000, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8274, Train: 0.9000, Val: 0.7840\n",
      "Epoch 38, Loss: 1.8256, Train: 0.9000, Val: 0.7840\n",
      "Epoch 39, Loss: 1.8238, Train: 0.9000, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8221, Train: 0.9000, Val: 0.7840\n",
      "Epoch 41, Loss: 1.8204, Train: 0.8929, Val: 0.7840\n",
      "Epoch 42, Loss: 1.8187, Train: 0.8929, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8171, Train: 0.8929, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8155, Train: 0.8929, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8139, Train: 0.8929, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8124, Train: 0.8929, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8109, Train: 0.8929, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8094, Train: 0.8929, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8080, Train: 0.8929, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8066, Train: 0.8929, Val: 0.7800\n",
      "Training time: 1.4126s, avg: 0.0283\n",
      "Forward: 0.006382673999999042\n",
      "Backward: 0.013223315999997567\n",
      "Test: 0.7790\n",
      "110 110\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "110\n",
      "Epoch 01, Loss: 1.9461, Train: 0.5857, Val: 0.5000\n",
      "Epoch 02, Loss: 1.9408, Train: 0.4429, Val: 0.2240\n",
      "Epoch 03, Loss: 1.9355, Train: 0.4857, Val: 0.2960\n",
      "Epoch 04, Loss: 1.9304, Train: 0.7214, Val: 0.4280\n",
      "Epoch 05, Loss: 1.9254, Train: 0.7500, Val: 0.5120\n",
      "Epoch 06, Loss: 1.9205, Train: 0.7929, Val: 0.5620\n",
      "Epoch 07, Loss: 1.9157, Train: 0.8286, Val: 0.6400\n",
      "Epoch 08, Loss: 1.9111, Train: 0.8857, Val: 0.7200\n",
      "Epoch 09, Loss: 1.9067, Train: 0.9214, Val: 0.7720\n",
      "Epoch 10, Loss: 1.9024, Train: 0.9214, Val: 0.7800\n",
      "Epoch 11, Loss: 1.8983, Train: 0.9143, Val: 0.7820\n",
      "Epoch 12, Loss: 1.8943, Train: 0.9071, Val: 0.7820\n",
      "Epoch 13, Loss: 1.8904, Train: 0.9071, Val: 0.7800\n",
      "Epoch 14, Loss: 1.8867, Train: 0.9071, Val: 0.7740\n",
      "Epoch 15, Loss: 1.8832, Train: 0.9071, Val: 0.7780\n",
      "Epoch 16, Loss: 1.8797, Train: 0.9071, Val: 0.7800\n",
      "Epoch 17, Loss: 1.8764, Train: 0.9000, Val: 0.7820\n",
      "Epoch 18, Loss: 1.8732, Train: 0.8929, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9000, Val: 0.7840\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9000, Val: 0.7820\n",
      "Epoch 21, Loss: 1.8642, Train: 0.9000, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9000, Val: 0.7880\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9000, Val: 0.7880\n",
      "Epoch 24, Loss: 1.8560, Train: 0.9000, Val: 0.7880\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9000, Val: 0.7880\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9000, Val: 0.7920\n",
      "Epoch 27, Loss: 1.8485, Train: 0.9000, Val: 0.7880\n",
      "Epoch 28, Loss: 1.8462, Train: 0.9071, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8439, Train: 0.9071, Val: 0.7860\n",
      "Epoch 30, Loss: 1.8417, Train: 0.9000, Val: 0.7860\n",
      "Epoch 31, Loss: 1.8395, Train: 0.8929, Val: 0.7820\n",
      "Epoch 32, Loss: 1.8374, Train: 0.8929, Val: 0.7820\n",
      "Epoch 33, Loss: 1.8353, Train: 0.8929, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8333, Train: 0.9071, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8313, Train: 0.9000, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8294, Train: 0.9000, Val: 0.7880\n",
      "Epoch 37, Loss: 1.8276, Train: 0.9000, Val: 0.7860\n",
      "Epoch 38, Loss: 1.8257, Train: 0.8929, Val: 0.7880\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8929, Val: 0.7880\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8786, Val: 0.7860\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8786, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8786, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8786, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8141, Train: 0.8786, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.4063s, avg: 0.0281\n",
      "Forward: 0.006213718000004746\n",
      "Backward: 0.013214643999998544\n",
      "Test: 0.7760\n",
      "111 111\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "111\n",
      "Epoch 01, Loss: 1.9461, Train: 0.2071, Val: 0.1020\n",
      "Epoch 02, Loss: 1.9408, Train: 0.7571, Val: 0.5380\n",
      "Epoch 03, Loss: 1.9356, Train: 0.8000, Val: 0.6460\n",
      "Epoch 04, Loss: 1.9305, Train: 0.8286, Val: 0.7100\n",
      "Epoch 05, Loss: 1.9254, Train: 0.8857, Val: 0.7280\n",
      "Epoch 06, Loss: 1.9206, Train: 0.8857, Val: 0.7460\n",
      "Epoch 07, Loss: 1.9158, Train: 0.8786, Val: 0.7240\n",
      "Epoch 08, Loss: 1.9112, Train: 0.8857, Val: 0.7460\n",
      "Epoch 09, Loss: 1.9068, Train: 0.9000, Val: 0.7620\n",
      "Epoch 10, Loss: 1.9025, Train: 0.9071, Val: 0.7660\n",
      "Epoch 11, Loss: 1.8984, Train: 0.9071, Val: 0.7660\n",
      "Epoch 12, Loss: 1.8944, Train: 0.9071, Val: 0.7680\n",
      "Epoch 13, Loss: 1.8906, Train: 0.9071, Val: 0.7700\n",
      "Epoch 14, Loss: 1.8869, Train: 0.9071, Val: 0.7680\n",
      "Epoch 15, Loss: 1.8833, Train: 0.9071, Val: 0.7680\n",
      "Epoch 16, Loss: 1.8799, Train: 0.9071, Val: 0.7720\n",
      "Epoch 17, Loss: 1.8765, Train: 0.9143, Val: 0.7800\n",
      "Epoch 18, Loss: 1.8733, Train: 0.9143, Val: 0.7820\n",
      "Epoch 19, Loss: 1.8702, Train: 0.9071, Val: 0.7800\n",
      "Epoch 20, Loss: 1.8672, Train: 0.9071, Val: 0.7860\n",
      "Epoch 21, Loss: 1.8643, Train: 0.9000, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8615, Train: 0.9000, Val: 0.7800\n",
      "Epoch 23, Loss: 1.8588, Train: 0.9071, Val: 0.7780\n",
      "Epoch 24, Loss: 1.8561, Train: 0.9071, Val: 0.7780\n",
      "Epoch 25, Loss: 1.8536, Train: 0.9000, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8511, Train: 0.9071, Val: 0.7820\n",
      "Epoch 27, Loss: 1.8487, Train: 0.9071, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8463, Train: 0.9000, Val: 0.7800\n",
      "Epoch 29, Loss: 1.8440, Train: 0.9000, Val: 0.7820\n",
      "Epoch 30, Loss: 1.8418, Train: 0.9000, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8396, Train: 0.9000, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8375, Train: 0.9000, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8354, Train: 0.9000, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8334, Train: 0.9000, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8315, Train: 0.9000, Val: 0.7820\n",
      "Epoch 36, Loss: 1.8296, Train: 0.8929, Val: 0.7800\n",
      "Epoch 37, Loss: 1.8277, Train: 0.8929, Val: 0.7800\n",
      "Epoch 38, Loss: 1.8259, Train: 0.8929, Val: 0.7800\n",
      "Epoch 39, Loss: 1.8241, Train: 0.8929, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8223, Train: 0.8929, Val: 0.7840\n",
      "Epoch 41, Loss: 1.8206, Train: 0.8929, Val: 0.7840\n",
      "Epoch 42, Loss: 1.8190, Train: 0.8929, Val: 0.7820\n",
      "Epoch 43, Loss: 1.8173, Train: 0.8929, Val: 0.7820\n",
      "Epoch 44, Loss: 1.8157, Train: 0.8929, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8142, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8126, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8111, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8097, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8082, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8068, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.4861s, avg: 0.0297\n",
      "Forward: 0.006413443999996389\n",
      "Backward: 0.01471446399999877\n",
      "Test: 0.7780\n",
      "112 112\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "112\n",
      "Epoch 01, Loss: 1.9460, Train: 0.3643, Val: 0.1880\n",
      "Epoch 02, Loss: 1.9406, Train: 0.7214, Val: 0.4840\n",
      "Epoch 03, Loss: 1.9353, Train: 0.7286, Val: 0.5780\n",
      "Epoch 04, Loss: 1.9302, Train: 0.7429, Val: 0.6400\n",
      "Epoch 05, Loss: 1.9252, Train: 0.7714, Val: 0.6860\n",
      "Epoch 06, Loss: 1.9203, Train: 0.8143, Val: 0.7360\n",
      "Epoch 07, Loss: 1.9156, Train: 0.8429, Val: 0.7780\n",
      "Epoch 08, Loss: 1.9110, Train: 0.8786, Val: 0.7840\n",
      "Epoch 09, Loss: 1.9065, Train: 0.8857, Val: 0.7780\n",
      "Epoch 10, Loss: 1.9022, Train: 0.8786, Val: 0.7660\n",
      "Epoch 11, Loss: 1.8981, Train: 0.8857, Val: 0.7580\n",
      "Epoch 12, Loss: 1.8941, Train: 0.9000, Val: 0.7420\n",
      "Epoch 13, Loss: 1.8903, Train: 0.9071, Val: 0.7500\n",
      "Epoch 14, Loss: 1.8866, Train: 0.9143, Val: 0.7640\n",
      "Epoch 15, Loss: 1.8830, Train: 0.9214, Val: 0.7700\n",
      "Epoch 16, Loss: 1.8796, Train: 0.9143, Val: 0.7680\n",
      "Epoch 17, Loss: 1.8763, Train: 0.9143, Val: 0.7760\n",
      "Epoch 18, Loss: 1.8731, Train: 0.9143, Val: 0.7800\n",
      "Epoch 19, Loss: 1.8700, Train: 0.9143, Val: 0.7840\n",
      "Epoch 20, Loss: 1.8670, Train: 0.9071, Val: 0.7840\n",
      "Epoch 21, Loss: 1.8641, Train: 0.9000, Val: 0.7900\n",
      "Epoch 22, Loss: 1.8613, Train: 0.9000, Val: 0.7880\n",
      "Epoch 23, Loss: 1.8586, Train: 0.9071, Val: 0.7840\n",
      "Epoch 24, Loss: 1.8559, Train: 0.8929, Val: 0.7800\n",
      "Epoch 25, Loss: 1.8534, Train: 0.8929, Val: 0.7720\n",
      "Epoch 26, Loss: 1.8509, Train: 0.9071, Val: 0.7760\n",
      "Epoch 27, Loss: 1.8485, Train: 0.9071, Val: 0.7820\n",
      "Epoch 28, Loss: 1.8461, Train: 0.9000, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8438, Train: 0.9000, Val: 0.7820\n",
      "Epoch 30, Loss: 1.8416, Train: 0.9000, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8394, Train: 0.9000, Val: 0.7820\n",
      "Epoch 32, Loss: 1.8373, Train: 0.9000, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8353, Train: 0.8929, Val: 0.7820\n",
      "Epoch 34, Loss: 1.8333, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8313, Train: 0.8929, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8294, Train: 0.8929, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8275, Train: 0.8929, Val: 0.7800\n",
      "Epoch 38, Loss: 1.8257, Train: 0.8786, Val: 0.7800\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8786, Val: 0.7780\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8929, Val: 0.7800\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8929, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8929, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8929, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8929, Val: 0.7840\n",
      "Epoch 45, Loss: 1.8141, Train: 0.8929, Val: 0.7840\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8929, Val: 0.7840\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8929, Val: 0.7840\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8929, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.4675s, avg: 0.0293\n",
      "Forward: 0.006654734000002236\n",
      "Backward: 0.013562325999999984\n",
      "Test: 0.7730\n",
      "113 113\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "113\n",
      "Epoch 01, Loss: 1.9460, Train: 0.4857, Val: 0.3480\n",
      "Epoch 02, Loss: 1.9407, Train: 0.5500, Val: 0.3380\n",
      "Epoch 03, Loss: 1.9355, Train: 0.6714, Val: 0.4180\n",
      "Epoch 04, Loss: 1.9304, Train: 0.7714, Val: 0.5480\n",
      "Epoch 05, Loss: 1.9253, Train: 0.8643, Val: 0.6560\n",
      "Epoch 06, Loss: 1.9204, Train: 0.8929, Val: 0.7060\n",
      "Epoch 07, Loss: 1.9157, Train: 0.9071, Val: 0.7600\n",
      "Epoch 08, Loss: 1.9111, Train: 0.9000, Val: 0.7840\n",
      "Epoch 09, Loss: 1.9067, Train: 0.8929, Val: 0.8040\n",
      "Epoch 10, Loss: 1.9024, Train: 0.9071, Val: 0.7920\n",
      "Epoch 11, Loss: 1.8983, Train: 0.9071, Val: 0.8020\n",
      "Epoch 12, Loss: 1.8943, Train: 0.9000, Val: 0.7740\n",
      "Epoch 13, Loss: 1.8905, Train: 0.8929, Val: 0.7720\n",
      "Epoch 14, Loss: 1.8868, Train: 0.9071, Val: 0.7740\n",
      "Epoch 15, Loss: 1.8832, Train: 0.9000, Val: 0.7720\n",
      "Epoch 16, Loss: 1.8797, Train: 0.9000, Val: 0.7660\n",
      "Epoch 17, Loss: 1.8764, Train: 0.9000, Val: 0.7640\n",
      "Epoch 18, Loss: 1.8732, Train: 0.9143, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9071, Val: 0.7800\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9071, Val: 0.7800\n",
      "Epoch 21, Loss: 1.8642, Train: 0.9071, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9000, Val: 0.7840\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9071, Val: 0.7900\n",
      "Epoch 24, Loss: 1.8560, Train: 0.9071, Val: 0.7940\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9071, Val: 0.7880\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9000, Val: 0.7880\n",
      "Epoch 27, Loss: 1.8485, Train: 0.9000, Val: 0.7880\n",
      "Epoch 28, Loss: 1.8462, Train: 0.9000, Val: 0.7840\n",
      "Epoch 29, Loss: 1.8439, Train: 0.9071, Val: 0.7820\n",
      "Epoch 30, Loss: 1.8417, Train: 0.9071, Val: 0.7800\n",
      "Epoch 31, Loss: 1.8395, Train: 0.9000, Val: 0.7800\n",
      "Epoch 32, Loss: 1.8374, Train: 0.8929, Val: 0.7800\n",
      "Epoch 33, Loss: 1.8353, Train: 0.9000, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8333, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8314, Train: 0.8929, Val: 0.7860\n",
      "Epoch 36, Loss: 1.8295, Train: 0.8929, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8276, Train: 0.8929, Val: 0.7840\n",
      "Epoch 38, Loss: 1.8258, Train: 0.8786, Val: 0.7840\n",
      "Epoch 39, Loss: 1.8240, Train: 0.8786, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8786, Val: 0.7800\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8786, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8189, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8786, Val: 0.7820\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8786, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8141, Train: 0.8786, Val: 0.7820\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8786, Val: 0.7820\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8786, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8786, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8786, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8786, Val: 0.7800\n",
      "Training time: 1.3828s, avg: 0.0277\n",
      "Forward: 0.006086038000000826\n",
      "Backward: 0.01353091399999812\n",
      "Test: 0.7750\n",
      "114 114\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "114\n",
      "Epoch 01, Loss: 1.9461, Train: 0.4286, Val: 0.3500\n",
      "Epoch 02, Loss: 1.9407, Train: 0.7286, Val: 0.5180\n",
      "Epoch 03, Loss: 1.9354, Train: 0.6500, Val: 0.4820\n",
      "Epoch 04, Loss: 1.9303, Train: 0.6500, Val: 0.5020\n",
      "Epoch 05, Loss: 1.9253, Train: 0.7500, Val: 0.5300\n",
      "Epoch 06, Loss: 1.9204, Train: 0.8071, Val: 0.6000\n",
      "Epoch 07, Loss: 1.9157, Train: 0.8286, Val: 0.6740\n",
      "Epoch 08, Loss: 1.9111, Train: 0.8857, Val: 0.7540\n",
      "Epoch 09, Loss: 1.9066, Train: 0.8929, Val: 0.7700\n",
      "Epoch 10, Loss: 1.9023, Train: 0.9000, Val: 0.7880\n",
      "Epoch 11, Loss: 1.8982, Train: 0.8857, Val: 0.7980\n",
      "Epoch 12, Loss: 1.8942, Train: 0.8929, Val: 0.7940\n",
      "Epoch 13, Loss: 1.8904, Train: 0.9000, Val: 0.8020\n",
      "Epoch 14, Loss: 1.8867, Train: 0.9000, Val: 0.7980\n",
      "Epoch 15, Loss: 1.8831, Train: 0.9071, Val: 0.7880\n",
      "Epoch 16, Loss: 1.8797, Train: 0.9071, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8763, Train: 0.9071, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8731, Train: 0.8929, Val: 0.7660\n",
      "Epoch 19, Loss: 1.8700, Train: 0.8857, Val: 0.7660\n",
      "Epoch 20, Loss: 1.8670, Train: 0.8857, Val: 0.7640\n",
      "Epoch 21, Loss: 1.8641, Train: 0.8786, Val: 0.7660\n",
      "Epoch 22, Loss: 1.8613, Train: 0.8929, Val: 0.7720\n",
      "Epoch 23, Loss: 1.8585, Train: 0.8929, Val: 0.7720\n",
      "Epoch 24, Loss: 1.8559, Train: 0.9071, Val: 0.7800\n",
      "Epoch 25, Loss: 1.8533, Train: 0.9071, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8508, Train: 0.9000, Val: 0.7860\n",
      "Epoch 27, Loss: 1.8484, Train: 0.9000, Val: 0.7900\n",
      "Epoch 28, Loss: 1.8461, Train: 0.9071, Val: 0.7900\n",
      "Epoch 29, Loss: 1.8438, Train: 0.9071, Val: 0.7900\n",
      "Epoch 30, Loss: 1.8416, Train: 0.9000, Val: 0.7840\n",
      "Epoch 31, Loss: 1.8394, Train: 0.8929, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8373, Train: 0.8857, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8352, Train: 0.8857, Val: 0.7880\n",
      "Epoch 34, Loss: 1.8332, Train: 0.8786, Val: 0.7860\n",
      "Epoch 35, Loss: 1.8313, Train: 0.8786, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8294, Train: 0.8786, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8275, Train: 0.8857, Val: 0.7860\n",
      "Epoch 38, Loss: 1.8257, Train: 0.8857, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8857, Val: 0.7860\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8929, Val: 0.7900\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8929, Val: 0.7900\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8929, Val: 0.7840\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8929, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8929, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8140, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8095, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.4575s, avg: 0.0292\n",
      "Forward: 0.006525292000000036\n",
      "Backward: 0.013935294000004888\n",
      "Test: 0.7800\n",
      "115 115\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "115\n",
      "Epoch 01, Loss: 1.9460, Train: 0.1571, Val: 0.1640\n",
      "Epoch 02, Loss: 1.9406, Train: 0.6143, Val: 0.3880\n",
      "Epoch 03, Loss: 1.9354, Train: 0.5000, Val: 0.2860\n",
      "Epoch 04, Loss: 1.9302, Train: 0.5643, Val: 0.3380\n",
      "Epoch 05, Loss: 1.9252, Train: 0.7071, Val: 0.4660\n",
      "Epoch 06, Loss: 1.9203, Train: 0.7929, Val: 0.6120\n",
      "Epoch 07, Loss: 1.9156, Train: 0.8429, Val: 0.7020\n",
      "Epoch 08, Loss: 1.9110, Train: 0.8786, Val: 0.7480\n",
      "Epoch 09, Loss: 1.9065, Train: 0.8929, Val: 0.7720\n",
      "Epoch 10, Loss: 1.9022, Train: 0.9071, Val: 0.7960\n",
      "Epoch 11, Loss: 1.8981, Train: 0.9000, Val: 0.7920\n",
      "Epoch 12, Loss: 1.8941, Train: 0.9143, Val: 0.7900\n",
      "Epoch 13, Loss: 1.8902, Train: 0.9143, Val: 0.7800\n",
      "Epoch 14, Loss: 1.8865, Train: 0.9071, Val: 0.7600\n",
      "Epoch 15, Loss: 1.8830, Train: 0.9000, Val: 0.7480\n",
      "Epoch 16, Loss: 1.8795, Train: 0.9000, Val: 0.7520\n",
      "Epoch 17, Loss: 1.8762, Train: 0.8857, Val: 0.7560\n",
      "Epoch 18, Loss: 1.8730, Train: 0.9000, Val: 0.7680\n",
      "Epoch 19, Loss: 1.8699, Train: 0.9000, Val: 0.7780\n",
      "Epoch 20, Loss: 1.8669, Train: 0.9000, Val: 0.7760\n",
      "Epoch 21, Loss: 1.8640, Train: 0.8929, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8611, Train: 0.9000, Val: 0.7860\n",
      "Epoch 23, Loss: 1.8584, Train: 0.9000, Val: 0.7920\n",
      "Epoch 24, Loss: 1.8558, Train: 0.9000, Val: 0.7940\n",
      "Epoch 25, Loss: 1.8532, Train: 0.9000, Val: 0.7900\n",
      "Epoch 26, Loss: 1.8507, Train: 0.9000, Val: 0.7880\n",
      "Epoch 27, Loss: 1.8483, Train: 0.9071, Val: 0.7840\n",
      "Epoch 28, Loss: 1.8460, Train: 0.9000, Val: 0.7820\n",
      "Epoch 29, Loss: 1.8437, Train: 0.9000, Val: 0.7860\n",
      "Epoch 30, Loss: 1.8414, Train: 0.8929, Val: 0.7860\n",
      "Epoch 31, Loss: 1.8393, Train: 0.8929, Val: 0.7880\n",
      "Epoch 32, Loss: 1.8372, Train: 0.8929, Val: 0.7900\n",
      "Epoch 33, Loss: 1.8351, Train: 0.8929, Val: 0.7900\n",
      "Epoch 34, Loss: 1.8331, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8311, Train: 0.8929, Val: 0.7880\n",
      "Epoch 36, Loss: 1.8292, Train: 0.9000, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8274, Train: 0.9000, Val: 0.7840\n",
      "Epoch 38, Loss: 1.8255, Train: 0.9000, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8238, Train: 0.8929, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8220, Train: 0.8929, Val: 0.7780\n",
      "Epoch 41, Loss: 1.8203, Train: 0.8929, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8186, Train: 0.8857, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8170, Train: 0.8857, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8154, Train: 0.8857, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8139, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8123, Train: 0.8857, Val: 0.7820\n",
      "Epoch 47, Loss: 1.8108, Train: 0.8857, Val: 0.7820\n",
      "Epoch 48, Loss: 1.8094, Train: 0.8857, Val: 0.7820\n",
      "Epoch 49, Loss: 1.8079, Train: 0.8857, Val: 0.7820\n",
      "Epoch 50, Loss: 1.8065, Train: 0.8857, Val: 0.7780\n",
      "Training time: 1.4472s, avg: 0.0289\n",
      "Forward: 0.006480315999998538\n",
      "Backward: 0.013444201999997176\n",
      "Test: 0.7760\n",
      "116 116\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "116\n",
      "Epoch 01, Loss: 1.9461, Train: 0.4929, Val: 0.2860\n",
      "Epoch 02, Loss: 1.9407, Train: 0.5643, Val: 0.2740\n",
      "Epoch 03, Loss: 1.9355, Train: 0.5500, Val: 0.3240\n",
      "Epoch 04, Loss: 1.9304, Train: 0.6500, Val: 0.4560\n",
      "Epoch 05, Loss: 1.9254, Train: 0.8143, Val: 0.6460\n",
      "Epoch 06, Loss: 1.9205, Train: 0.8857, Val: 0.7380\n",
      "Epoch 07, Loss: 1.9158, Train: 0.9143, Val: 0.7940\n",
      "Epoch 08, Loss: 1.9112, Train: 0.8857, Val: 0.7780\n",
      "Epoch 09, Loss: 1.9067, Train: 0.8929, Val: 0.7700\n",
      "Epoch 10, Loss: 1.9024, Train: 0.8786, Val: 0.7520\n",
      "Epoch 11, Loss: 1.8983, Train: 0.9000, Val: 0.7560\n",
      "Epoch 12, Loss: 1.8943, Train: 0.9143, Val: 0.7680\n",
      "Epoch 13, Loss: 1.8905, Train: 0.9071, Val: 0.7720\n",
      "Epoch 14, Loss: 1.8868, Train: 0.9000, Val: 0.7720\n",
      "Epoch 15, Loss: 1.8832, Train: 0.8929, Val: 0.7660\n",
      "Epoch 16, Loss: 1.8798, Train: 0.8857, Val: 0.7720\n",
      "Epoch 17, Loss: 1.8764, Train: 0.8929, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8732, Train: 0.8929, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8701, Train: 0.9071, Val: 0.7860\n",
      "Epoch 20, Loss: 1.8671, Train: 0.9071, Val: 0.7820\n",
      "Epoch 21, Loss: 1.8642, Train: 0.9143, Val: 0.7780\n",
      "Epoch 22, Loss: 1.8614, Train: 0.9143, Val: 0.7760\n",
      "Epoch 23, Loss: 1.8587, Train: 0.9071, Val: 0.7740\n",
      "Epoch 24, Loss: 1.8561, Train: 0.9071, Val: 0.7720\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9000, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9000, Val: 0.7840\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9071, Val: 0.7840\n",
      "Epoch 28, Loss: 1.8462, Train: 0.8929, Val: 0.7860\n",
      "Epoch 29, Loss: 1.8439, Train: 0.8929, Val: 0.7860\n",
      "Epoch 30, Loss: 1.8417, Train: 0.8929, Val: 0.7820\n",
      "Epoch 31, Loss: 1.8395, Train: 0.8929, Val: 0.7800\n",
      "Epoch 32, Loss: 1.8374, Train: 0.9000, Val: 0.7800\n",
      "Epoch 33, Loss: 1.8353, Train: 0.9000, Val: 0.7800\n",
      "Epoch 34, Loss: 1.8333, Train: 0.9000, Val: 0.7800\n",
      "Epoch 35, Loss: 1.8314, Train: 0.9000, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8294, Train: 0.9000, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8276, Train: 0.8929, Val: 0.7860\n",
      "Epoch 38, Loss: 1.8257, Train: 0.8929, Val: 0.7860\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8857, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8857, Val: 0.7820\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8786, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8786, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8857, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8857, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8140, Train: 0.8929, Val: 0.7820\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8929, Val: 0.7840\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8929, Val: 0.7840\n",
      "Epoch 48, Loss: 1.8096, Train: 0.8929, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7780\n",
      "Training time: 1.4272s, avg: 0.0285\n",
      "Forward: 0.00637790199999472\n",
      "Backward: 0.013464551999988998\n",
      "Test: 0.7750\n",
      "117 117\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "117\n",
      "Epoch 01, Loss: 1.9458, Train: 0.2714, Val: 0.1360\n",
      "Epoch 02, Loss: 1.9405, Train: 0.4714, Val: 0.2440\n",
      "Epoch 03, Loss: 1.9353, Train: 0.7071, Val: 0.5000\n",
      "Epoch 04, Loss: 1.9301, Train: 0.8286, Val: 0.7160\n",
      "Epoch 05, Loss: 1.9251, Train: 0.8786, Val: 0.7540\n",
      "Epoch 06, Loss: 1.9202, Train: 0.8786, Val: 0.7580\n",
      "Epoch 07, Loss: 1.9155, Train: 0.8929, Val: 0.7420\n",
      "Epoch 08, Loss: 1.9109, Train: 0.9000, Val: 0.7340\n",
      "Epoch 09, Loss: 1.9064, Train: 0.9071, Val: 0.7540\n",
      "Epoch 10, Loss: 1.9021, Train: 0.9071, Val: 0.7760\n",
      "Epoch 11, Loss: 1.8980, Train: 0.9000, Val: 0.7920\n",
      "Epoch 12, Loss: 1.8940, Train: 0.9000, Val: 0.7940\n",
      "Epoch 13, Loss: 1.8902, Train: 0.8929, Val: 0.7880\n",
      "Epoch 14, Loss: 1.8865, Train: 0.8929, Val: 0.7780\n",
      "Epoch 15, Loss: 1.8829, Train: 0.8929, Val: 0.7800\n",
      "Epoch 16, Loss: 1.8795, Train: 0.8929, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8762, Train: 0.9071, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8730, Train: 0.9071, Val: 0.7780\n",
      "Epoch 19, Loss: 1.8699, Train: 0.9071, Val: 0.7760\n",
      "Epoch 20, Loss: 1.8669, Train: 0.9071, Val: 0.7740\n",
      "Epoch 21, Loss: 1.8640, Train: 0.9071, Val: 0.7800\n",
      "Epoch 22, Loss: 1.8612, Train: 0.9071, Val: 0.7900\n",
      "Epoch 23, Loss: 1.8584, Train: 0.9071, Val: 0.7860\n",
      "Epoch 24, Loss: 1.8558, Train: 0.9071, Val: 0.7840\n",
      "Epoch 25, Loss: 1.8532, Train: 0.9071, Val: 0.7840\n",
      "Epoch 26, Loss: 1.8507, Train: 0.9071, Val: 0.7860\n",
      "Epoch 27, Loss: 1.8483, Train: 0.9000, Val: 0.7800\n",
      "Epoch 28, Loss: 1.8459, Train: 0.9000, Val: 0.7860\n",
      "Epoch 29, Loss: 1.8437, Train: 0.9000, Val: 0.7880\n",
      "Epoch 30, Loss: 1.8414, Train: 0.9000, Val: 0.7840\n",
      "Epoch 31, Loss: 1.8393, Train: 0.9071, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8371, Train: 0.9000, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8351, Train: 0.9000, Val: 0.7860\n",
      "Epoch 34, Loss: 1.8331, Train: 0.9000, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8311, Train: 0.9000, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8292, Train: 0.8929, Val: 0.7860\n",
      "Epoch 37, Loss: 1.8273, Train: 0.8857, Val: 0.7840\n",
      "Epoch 38, Loss: 1.8255, Train: 0.8786, Val: 0.7840\n",
      "Epoch 39, Loss: 1.8237, Train: 0.8786, Val: 0.7840\n",
      "Epoch 40, Loss: 1.8220, Train: 0.8786, Val: 0.7840\n",
      "Epoch 41, Loss: 1.8203, Train: 0.8786, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8186, Train: 0.8857, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8170, Train: 0.8857, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8154, Train: 0.8929, Val: 0.7820\n",
      "Epoch 45, Loss: 1.8138, Train: 0.8857, Val: 0.7820\n",
      "Epoch 46, Loss: 1.8123, Train: 0.8857, Val: 0.7820\n",
      "Epoch 47, Loss: 1.8108, Train: 0.8857, Val: 0.7820\n",
      "Epoch 48, Loss: 1.8093, Train: 0.8786, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8079, Train: 0.8786, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8065, Train: 0.8786, Val: 0.7800\n",
      "Training time: 1.3474s, avg: 0.0269\n",
      "Forward: 0.006169096000007812\n",
      "Backward: 0.012665051999997559\n",
      "Test: 0.7820\n",
      "118 118\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "118\n",
      "Epoch 01, Loss: 1.9459, Train: 0.5500, Val: 0.3500\n",
      "Epoch 02, Loss: 1.9406, Train: 0.6143, Val: 0.3880\n",
      "Epoch 03, Loss: 1.9354, Train: 0.6571, Val: 0.4340\n",
      "Epoch 04, Loss: 1.9302, Train: 0.8071, Val: 0.5540\n",
      "Epoch 05, Loss: 1.9252, Train: 0.8714, Val: 0.6860\n",
      "Epoch 06, Loss: 1.9203, Train: 0.9143, Val: 0.7500\n",
      "Epoch 07, Loss: 1.9156, Train: 0.9000, Val: 0.7760\n",
      "Epoch 08, Loss: 1.9110, Train: 0.9000, Val: 0.7760\n",
      "Epoch 09, Loss: 1.9066, Train: 0.8857, Val: 0.7760\n",
      "Epoch 10, Loss: 1.9023, Train: 0.8786, Val: 0.7720\n",
      "Epoch 11, Loss: 1.8981, Train: 0.8786, Val: 0.7760\n",
      "Epoch 12, Loss: 1.8942, Train: 0.8929, Val: 0.7820\n",
      "Epoch 13, Loss: 1.8903, Train: 0.9143, Val: 0.7720\n",
      "Epoch 14, Loss: 1.8866, Train: 0.9143, Val: 0.7820\n",
      "Epoch 15, Loss: 1.8831, Train: 0.9214, Val: 0.7780\n",
      "Epoch 16, Loss: 1.8796, Train: 0.9143, Val: 0.7780\n",
      "Epoch 17, Loss: 1.8763, Train: 0.9143, Val: 0.7780\n",
      "Epoch 18, Loss: 1.8731, Train: 0.9071, Val: 0.7840\n",
      "Epoch 19, Loss: 1.8700, Train: 0.9071, Val: 0.7800\n",
      "Epoch 20, Loss: 1.8670, Train: 0.9000, Val: 0.7860\n",
      "Epoch 21, Loss: 1.8641, Train: 0.8929, Val: 0.7880\n",
      "Epoch 22, Loss: 1.8612, Train: 0.9000, Val: 0.7860\n",
      "Epoch 23, Loss: 1.8585, Train: 0.9000, Val: 0.7820\n",
      "Epoch 24, Loss: 1.8559, Train: 0.9000, Val: 0.7780\n",
      "Epoch 25, Loss: 1.8533, Train: 0.9000, Val: 0.7800\n",
      "Epoch 26, Loss: 1.8508, Train: 0.9000, Val: 0.7860\n",
      "Epoch 27, Loss: 1.8484, Train: 0.9000, Val: 0.7800\n",
      "Epoch 28, Loss: 1.8460, Train: 0.9000, Val: 0.7740\n",
      "Epoch 29, Loss: 1.8437, Train: 0.9071, Val: 0.7760\n",
      "Epoch 30, Loss: 1.8415, Train: 0.9071, Val: 0.7800\n",
      "Epoch 31, Loss: 1.8393, Train: 0.8929, Val: 0.7840\n",
      "Epoch 32, Loss: 1.8372, Train: 0.8929, Val: 0.7840\n",
      "Epoch 33, Loss: 1.8352, Train: 0.8929, Val: 0.7840\n",
      "Epoch 34, Loss: 1.8332, Train: 0.8929, Val: 0.7840\n",
      "Epoch 35, Loss: 1.8312, Train: 0.8929, Val: 0.7840\n",
      "Epoch 36, Loss: 1.8293, Train: 0.8929, Val: 0.7840\n",
      "Epoch 37, Loss: 1.8275, Train: 0.8929, Val: 0.7800\n",
      "Epoch 38, Loss: 1.8256, Train: 0.8929, Val: 0.7800\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8929, Val: 0.7800\n",
      "Epoch 40, Loss: 1.8221, Train: 0.8929, Val: 0.7800\n",
      "Epoch 41, Loss: 1.8204, Train: 0.8929, Val: 0.7800\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8929, Val: 0.7800\n",
      "Epoch 43, Loss: 1.8171, Train: 0.8929, Val: 0.7800\n",
      "Epoch 44, Loss: 1.8155, Train: 0.8857, Val: 0.7800\n",
      "Epoch 45, Loss: 1.8140, Train: 0.8857, Val: 0.7800\n",
      "Epoch 46, Loss: 1.8124, Train: 0.8857, Val: 0.7800\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8857, Val: 0.7800\n",
      "Epoch 48, Loss: 1.8095, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8080, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8066, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.3325s, avg: 0.0266\n",
      "Forward: 0.006082339999995839\n",
      "Backward: 0.012569788000002973\n",
      "Test: 0.7730\n",
      "119 119\n",
      "dict_keys([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
      "119\n",
      "Epoch 01, Loss: 1.9461, Train: 0.2214, Val: 0.0880\n",
      "Epoch 02, Loss: 1.9408, Train: 0.7714, Val: 0.5380\n",
      "Epoch 03, Loss: 1.9355, Train: 0.7714, Val: 0.5440\n",
      "Epoch 04, Loss: 1.9304, Train: 0.7857, Val: 0.6700\n",
      "Epoch 05, Loss: 1.9254, Train: 0.8000, Val: 0.7320\n",
      "Epoch 06, Loss: 1.9205, Train: 0.8429, Val: 0.7600\n",
      "Epoch 07, Loss: 1.9157, Train: 0.8571, Val: 0.7640\n",
      "Epoch 08, Loss: 1.9111, Train: 0.8714, Val: 0.7540\n",
      "Epoch 09, Loss: 1.9067, Train: 0.8786, Val: 0.7580\n",
      "Epoch 10, Loss: 1.9024, Train: 0.9000, Val: 0.7620\n",
      "Epoch 11, Loss: 1.8982, Train: 0.9071, Val: 0.7720\n",
      "Epoch 12, Loss: 1.8943, Train: 0.9071, Val: 0.7560\n",
      "Epoch 13, Loss: 1.8904, Train: 0.8929, Val: 0.7540\n",
      "Epoch 14, Loss: 1.8867, Train: 0.9071, Val: 0.7640\n",
      "Epoch 15, Loss: 1.8832, Train: 0.9071, Val: 0.7780\n",
      "Epoch 16, Loss: 1.8797, Train: 0.9143, Val: 0.7720\n",
      "Epoch 17, Loss: 1.8764, Train: 0.9071, Val: 0.7800\n",
      "Epoch 18, Loss: 1.8732, Train: 0.9071, Val: 0.7760\n",
      "Epoch 19, Loss: 1.8701, Train: 0.8929, Val: 0.7700\n",
      "Epoch 20, Loss: 1.8671, Train: 0.8929, Val: 0.7620\n",
      "Epoch 21, Loss: 1.8642, Train: 0.8929, Val: 0.7640\n",
      "Epoch 22, Loss: 1.8614, Train: 0.8857, Val: 0.7640\n",
      "Epoch 23, Loss: 1.8587, Train: 0.8929, Val: 0.7700\n",
      "Epoch 24, Loss: 1.8561, Train: 0.9000, Val: 0.7760\n",
      "Epoch 25, Loss: 1.8535, Train: 0.9000, Val: 0.7780\n",
      "Epoch 26, Loss: 1.8510, Train: 0.9071, Val: 0.7760\n",
      "Epoch 27, Loss: 1.8486, Train: 0.9071, Val: 0.7800\n",
      "Epoch 28, Loss: 1.8462, Train: 0.9071, Val: 0.7820\n",
      "Epoch 29, Loss: 1.8439, Train: 0.9071, Val: 0.7860\n",
      "Epoch 30, Loss: 1.8417, Train: 0.9071, Val: 0.7880\n",
      "Epoch 31, Loss: 1.8395, Train: 0.9000, Val: 0.7880\n",
      "Epoch 32, Loss: 1.8374, Train: 0.9000, Val: 0.7880\n",
      "Epoch 33, Loss: 1.8353, Train: 0.9000, Val: 0.7820\n",
      "Epoch 34, Loss: 1.8333, Train: 0.8929, Val: 0.7780\n",
      "Epoch 35, Loss: 1.8314, Train: 0.8857, Val: 0.7780\n",
      "Epoch 36, Loss: 1.8294, Train: 0.8857, Val: 0.7780\n",
      "Epoch 37, Loss: 1.8276, Train: 0.8786, Val: 0.7780\n",
      "Epoch 38, Loss: 1.8257, Train: 0.8786, Val: 0.7780\n",
      "Epoch 39, Loss: 1.8239, Train: 0.8857, Val: 0.7780\n",
      "Epoch 40, Loss: 1.8222, Train: 0.8857, Val: 0.7800\n",
      "Epoch 41, Loss: 1.8205, Train: 0.8929, Val: 0.7820\n",
      "Epoch 42, Loss: 1.8188, Train: 0.8929, Val: 0.7840\n",
      "Epoch 43, Loss: 1.8172, Train: 0.8929, Val: 0.7840\n",
      "Epoch 44, Loss: 1.8156, Train: 0.8929, Val: 0.7840\n",
      "Epoch 45, Loss: 1.8140, Train: 0.8857, Val: 0.7820\n",
      "Epoch 46, Loss: 1.8125, Train: 0.8857, Val: 0.7820\n",
      "Epoch 47, Loss: 1.8110, Train: 0.8857, Val: 0.7820\n",
      "Epoch 48, Loss: 1.8095, Train: 0.8857, Val: 0.7800\n",
      "Epoch 49, Loss: 1.8081, Train: 0.8857, Val: 0.7800\n",
      "Epoch 50, Loss: 1.8067, Train: 0.8857, Val: 0.7800\n",
      "Training time: 1.3560s, avg: 0.0271\n",
      "Forward: 0.0060111400000050705\n",
      "Backward: 0.0129750639999952\n",
      "Test: 0.7780\n"
     ]
    }
   ],
   "source": [
    "params = list(range(100,120))\n",
    "result = []\n",
    "for k in params:\n",
    "    model = DGC(in_feats, n_classes, k, T, device, is_linear=True)\n",
    "    train(model, graph, raw_features, label, split_idx[\"train_mask\"], split_idx[\"val_mask\"], lr, float(wd), epochs)\n",
    "    acc = test(model, graph, raw_features, label, split_idx[\"test_mask\"])\n",
    "    result.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.xlabel('K')\n",
    "plt.xticks(range(len(params)), params)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53236ea3850ed19be974e6741f9c7a7506ed085a4abb6419de9917a82e4ffe4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
